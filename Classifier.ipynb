{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. predict the unlabeled data (Naive Bayes, SVM)\\n2. evaluate the accuracy,f1,ruc,etc?? (check with the doc later)\\n3. Topic Modeling\\n4. wordcloud(maybe,if applicable)\\n5. visualization(maybe, if applicable)\\n6. think about some other fun questions about this data set\\n7. paper \\n    Exploring the Use of Text Classification in the Legal Domain - arXiv\\n    https://arxiv.org/pdf/1710.09306\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO\n",
    "\n",
    "\"\"\"\n",
    "1. predict the unlabeled data (Naive Bayes, SVM) XGBoost\n",
    "2. evaluate the accuracy ✓\n",
    "3. Topic Modeling\n",
    "4. wordcloud(maybe,if applicable)\n",
    "5. visualization(maybe, if applicable)\n",
    "6. think about some other fun questions about this data set\n",
    "7. paper \n",
    "    Exploring the Use of Text Classification in the Legal Domain - arXiv\n",
    "    https://arxiv.org/pdf/1710.09306\n",
    "8. reproducible     \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string \n",
    "import nltk\n",
    "import re \n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judgements</th>\n",
       "      <th>Area.of.Law</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LNIND_1988_CAL_114</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LNIND_1956_CAL_163</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LNIND_1976_CAL_277</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LNIND_1980_CAL_52</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LNIND_1955_CAL_124</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>LNIND_1993_DEL_112</td>\n",
       "      <td>Criminal Laws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>LNIND_1988_CAL_83</td>\n",
       "      <td>Service Law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>LNIND_1993_DEL_16</td>\n",
       "      <td>Criminal Laws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>LNIND_1957_CAL_46</td>\n",
       "      <td>Succession Laws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>LNIND_1954_CAL_141</td>\n",
       "      <td>Government Contracts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Judgements           Area.of.Law\n",
       "0    LNIND_1988_CAL_114          To be Tested\n",
       "1    LNIND_1956_CAL_163          To be Tested\n",
       "2    LNIND_1976_CAL_277          To be Tested\n",
       "3     LNIND_1980_CAL_52          To be Tested\n",
       "4    LNIND_1955_CAL_124          To be Tested\n",
       "..                  ...                   ...\n",
       "994  LNIND_1993_DEL_112         Criminal Laws\n",
       "995   LNIND_1988_CAL_83           Service Law\n",
       "996   LNIND_1993_DEL_16         Criminal Laws\n",
       "997   LNIND_1957_CAL_46       Succession Laws\n",
       "998  LNIND_1954_CAL_141  Government Contracts\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look on the labels' file\n",
    "mapping = pd.read_csv('data/Interview_Mapping.csv')\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# create labels\n",
    "unlabeled = []\n",
    "labeled = []\n",
    "labels = []\n",
    "\n",
    "for index,row in mapping.iterrows():\n",
    "    if row['Area.of.Law'] == 'To be Tested':\n",
    "        unlabeled.append(row['Judgements'])\n",
    "    else: \n",
    "        labeled.append(row['Judgements'])\n",
    "        labels.append(row['Area.of.Law'])\n",
    "        \n",
    "# how much unique area of law        \n",
    "print(len(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "import os\n",
    "\n",
    "unlabeled_text=[]\n",
    "labeled_text=[]\n",
    "\n",
    "for name in unlabeled:\n",
    "    path = os.path.join('data/',name+'.txt')\n",
    "    with open(path,'r',errors = 'ignore') as f:\n",
    "        unlabeled_text.append(f.read())\n",
    "for name in labeled:\n",
    "    path = os.path.join('data/',name+'.txt')\n",
    "    with open(path,'r',errors = 'ignore') as f:\n",
    "        labeled_text.append(f.read())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labeled_text[] each of the element in this array is a passage\n",
    "and what i need to do is to clean each of the passage first \n",
    "and combine the cleaned passages together into a new array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Flora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Flora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "\n",
    "labeled_cleaned=[]\n",
    "unlabeled_cleaned = []\n",
    "\n",
    "for passage in labeled_text:\n",
    "    # remove links \n",
    "    passage= re.sub(r'http(s)?:\\/\\/\\S*', \"\", str(passage))\n",
    "    # remove \\n\n",
    "    passage = ''.join([elem.replace('\\n',' ') for elem in passage])\n",
    "    # normalization and remove stopwords\n",
    "    passage = ' '.join([elem for elem in passage.lower().split() if elem not in stop])\n",
    "    #remove punctuation \n",
    "    passage = ''.join([elem.replace('[^\\w\\s]',' ') for elem in passage if elem not in punct])\n",
    "    #remove digits\n",
    "    passage = ''.join([elem for elem in passage if not elem.isdigit()])\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    passage = ' '.join(lemmatizer.lemmatize(elem) for elem in passage.split())\n",
    "    \n",
    "    labeled_cleaned.append(passage)\n",
    "\n",
    "    \n",
    "for passage in unlabeled_text:\n",
    "        \n",
    "    # remove links \n",
    "    passage= re.sub(r'http(s)?:\\/\\/\\S*', \"\", str(passage))\n",
    "    # remove \\n\n",
    "    passage = ''.join([elem.replace('\\n',' ') for elem in passage])\n",
    "    # normalization and remove stopwords\n",
    "    passage = ' '.join([elem for elem in passage.lower().split() if elem not in stop])\n",
    "    #remove punctuation \n",
    "    passage = ''.join([elem.replace('[^\\w\\s]',' ') for elem in passage if elem not in punct])\n",
    "    #remove digits\n",
    "    passage = ''.join([elem for elem in passage if not elem.isdigit()])\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    passage = ' '.join(lemmatizer.lemmatize(elem) for elem in passage.split())\n",
    "    unlabeled_cleaned.append(passage)\n",
    "    # it has to be a string so it could be processed later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the labeled data into training and validation set \n",
    "# use 7-3 \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_text,val_text,train_labels,val_labels =train_test_split(labeled_cleaned,labels,test_size = 0.25,random_state = 0)\n",
    "\n",
    "# do tfidf to get X_train and X_val\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_text)\n",
    "X_val = vectorizer.transform(val_text)\n",
    "\n",
    "# do tfidf to get X_test (unlabeled text that needed to be predicted)\n",
    "X_test = vectorizer.transform(unlabeled_cleaned) #transform on test set, not fit_transform\n",
    "\n",
    "# do label encoding to get y_train and y_val\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_val = encoder.transform(val_labels)\n",
    "all_labels = encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,classification_report\n",
    "\n",
    "def get_metrics(y_val, y_predicted,yHat_train,y_train):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_val, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_val, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_val, y_predicted, pos_label=None, average='weighted')\n",
    "    # true positives + true negatives/ total\n",
    "    accuracyTest = accuracy_score(y_val, y_predicted)\n",
    "    accuracyTrain = accuracy_score(y_train,yHat_train)\n",
    "    return accuracyTest,accuracyTrain, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  6  6  6  6  6  6  6 23  6 23  6 23 39  6  6  6 39  6  6 23 23  6  6\n",
      " 39  6 23  6 39 23  6 39  6 23  6 14  6  6  6  6  6  6  6 23  6  6  6  6\n",
      " 39  6  6 39 39  6  6  6  6  6  6  6 23  6  6  6  6  6  6  6  6  6  6 23\n",
      "  6  6  6  6 23  6  6  6  6 23  6 39  6  6  6  6  6 23 39 23 23  6  6  6\n",
      " 23  6  6  6]\n",
      "Test accuracy = 0.329, Train accuracy = 0.359,precision = 0.250, recall = 0.329, f1 = 0.214\n"
     ]
    }
   ],
   "source": [
    "# 1st model: Naive Bayes\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "def modelNB(X_train,y_train,X_val,X_test):\n",
    "    modelNB = naive_bayes.MultinomialNB()\n",
    "    modelNB.fit(X_train,y_train)\n",
    "    predicted_labels_ = modelNB.predict(X_val)\n",
    "    result_ = modelNB.predict(X_test)\n",
    "    print(result_)\n",
    "    yHat_train_ = modelNB.predict(X_train)\n",
    "    return predicted_labels_,result_,yHat_train_\n",
    "\n",
    "predicted_labels,result,yHat_train=modelNB(X_train,y_train,X_val,X_test)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Test accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 28  6  8 14  1  7 28 23  6 23 39 23 39  6 14 18 39 37 11 23 23 16  7\n",
      " 39 25 23  8 39 36 36 39 34 23 34 14  2 11 28 15 16  6 13 36 34 34 14 38\n",
      " 39 30 13 39 39 27  1 17  1 18  4 11 34 15 34 38 38 35 14 36  6  7 18 36\n",
      "  6 11 37  8 23 13 20 36  6  7  1 39  5 39 34 37 36 23 39 23 23 14 27 16\n",
      " 23 21 13  6]\n",
      "Test accuracy = 0.631, Train accuracy = 0.917,precision = 0.641, recall = 0.631, f1 = 0.623\n"
     ]
    }
   ],
   "source": [
    "# 2nd model: Logistic Regression\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def modelLR(X_train,y_train,X_val,X_test):\n",
    "    modelLR = LogisticRegression(C=3.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=0)\n",
    "    modelLR.fit(X_train,y_train)\n",
    "    predicted_labels_ = modelLR.predict(X_val)\n",
    "    result_ = modelLR.predict(X_test)\n",
    "    print(result_)\n",
    "    yHat_train_ = modelLR.predict(X_train)\n",
    "    return predicted_labels_,result_,yHat_train_\n",
    "\n",
    "predicted_labels,result,yHat_train=modelLR(X_train,y_train,X_val,X_test)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Test accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3nd model: SVM\n",
    "# before applying SVMs,  standardize the data first.\n",
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "from sklearn import decomposition,preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(X_train)\n",
    "X_train_svd = svd.transform(X_train)\n",
    "X_val_svd = svd.transform(X_val)\n",
    "X_test_svd=svd.transform(X_test)\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(X_train_svd)\n",
    "X_train_svd_scl = scl.transform(X_train_svd)\n",
    "X_val_svd_scl = scl.transform(X_val_svd)\n",
    "X_test_svd_scl = scl.transform(X_test_svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n",
      "Before SVD: \n",
      "Test accuracy = 0.182, Train accuracy = 0.141,precision = 0.033, recall = 0.182, f1 = 0.056\n",
      "[34  6  6 14  6  6  6 28 23  6 23 39 23 39  6 14  6 39 37  6 23 23  6  6\n",
      " 39  6 23  6 39 36 36 39 34 23 34 14  1 23 14 15 16  6 13 36  6 34 14  6\n",
      " 39 13 13 39 39  6  1 28  1  6  6  6  6  6 34 38 38  6  6 36  6  7  6 36\n",
      "  6  6 37  6 23 13 23 36  6  7  1 39  6 39 34 37 36 23 39 23 23 13  6  6\n",
      " 23 38 13  6]\n",
      "After SVD: \n",
      "Test accuracy = 0.573, Train accuracy = 0.866,precision = 0.521, recall = 0.573, f1 = 0.513\n"
     ]
    }
   ],
   "source": [
    "# use SVM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def modelSVM(X_train,y_train,X_val,X_test):\n",
    "    modelSVM = svm.SVC(C=1.0,probability=True)\n",
    "    modelSVM.fit(X_train,y_train)\n",
    "    predicted_labels_ = modelSVM.predict(X_val)\n",
    "    result_ = modelSVM.predict(X_test)\n",
    "    print(result_)\n",
    "    yHat_train_ = modelSVM.predict(X_train)\n",
    "    return predicted_labels_,result_,yHat_train_\n",
    "\n",
    "predicted_labels,result,yHat_train=modelSVM(X_train,y_train,X_val,X_test)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Before SVD: \\nTest accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n",
    "\n",
    "\n",
    "predicted_labels_SVD,result_SVD,yHat_train_SVD =modelSVM(X_train_svd_scl,y_train,X_val_svd_scl,X_test_svd_scl)\n",
    "accuracyTest_SVD,accuracyTrain_SVD, precision_SVD, recall_SVD, f1_SVD = get_metrics(y_val, predicted_labels_SVD,yHat_train_SVD,y_train)\n",
    "print(\"After SVD: \\nTest accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest_SVD,accuracyTrain_SVD, precision_SVD, recall_SVD, f1_SVD))\n",
    "\n",
    "# without truncatedSVD, the classifier was somehow underfit.\n",
    "# with truncatedSVD, the accuracy was raised dramatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 28  6  8 28  1 34 28 23  6 23 39 23 39  6 14 37 39 37 34 23 23 37  6\n",
      " 39  6 23  7 39  8 36 39  6 23 34 14  1 34 14 15 37  6 13 36  6 39 14  6\n",
      " 39 14 13 39 39  6  1  6  1 37  6  6 23 14 39 38 34 38 14 36 39 14 37 36\n",
      "  6  6 37  6 23 13  8 14  6  7  1 39 36 39 34 37 36 23 39 23 23 14  6 16\n",
      " 23 21 13  6]\n",
      "1\n",
      "Test accuracy = 0.613, Train accuracy = 0.929,precision = 0.552, recall = 0.613, f1 = 0.558\n"
     ]
    }
   ],
   "source": [
    "# 4th model: XgBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "def modelxgb(X_train,y_train,X_val,X_test):\n",
    "    modelxgb = xgb.XGBClassifier(max_depth=6, n_estimators=200, colsample_bytree=0.5, \n",
    "                        subsample=0.5, nthread=10, learning_rate=0.01)\n",
    "    modelxgb.fit(X_train,y_train)\n",
    "    predicted_labels_ = modelxgb.predict(X_val)\n",
    "    result_ = modelxgb.predict(X_test)\n",
    "    print(result_)\n",
    "    yHat_train_ = modelxgb.predict(X_train)\n",
    "    return predicted_labels_,result_,yHat_train_\n",
    "\n",
    "# see the result on tfidf data \n",
    "predicted_labels,result,yHat_train = modelxgb(X_train,y_train,X_val,X_test)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"1\\nTest accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[34 28  6  8 28  1  4 28 23  6 23  6 23 39  6 14 18 39 37  6 23 23 37  7\n",
    " 39  6 23  7 39  8 36 39  6 23 37 14  2 34 14 15 37  8 13 36 34 34 14  6\n",
    " 39 30 13 39 39  6  2 17  2 18  6  6 23 34 39 38 34 38 14 36 39  7 18 36\n",
    " 27  6 37  6 23 13  8 14  6  7  1 39  6 39 38 37 36 23 39 23 23 13 27 16\n",
    " 23 21 13  6]\n",
    "Before SVD: \n",
    "Test accuracy = 0.618, Train accuracy = 1.000,precision = 0.599, recall = 0.618, f1 = 0.591\n",
    "\n",
    "[34 28  6  8 14  1  6 28 23  6 23 39 23 39  6 14  6 39  6 34 23 23 37  6\n",
    " 39  6 23 28 39 36 36 39  6 23 34 14  1 11 14 15 16  6 13 36 34 34 14 38\n",
    " 39 14 13 39 39 27  1 28  1 18  6 11 23 14 34 38 38  6 14  8 39 14  6 36\n",
    "  6  6  6  6 23 13  6 14  6  7  1 39 28  6 34 37 36 23 39 23 23 14  6 16\n",
    " 23 38 13  6]\n",
    "After SVD: \n",
    "Test accuracy = 0.569, Train accuracy = 1.000,precision = 0.522, recall = 0.569, f1 = 0.525\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(Vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in Vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-98c8c62e30ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportance_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_most_important_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelLR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-d69485184398>\u001b[0m in \u001b[0;36mget_most_important_features\u001b[1;34m(Vectorizer, model, n)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# loop for each class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mclass_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mword_importances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0msorted_coeff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_importances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "importance_tfidf = get_most_important_features(vectorizer,modelLR,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Irrelevant', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Disaster', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "\n",
    "top_scores = [a[0] for a in importance_tfidf[1]['tops']]\n",
    "top_words = [a[1] for a in importance_tfidf[1]['tops']]\n",
    "bottom_scores = [a[0] for a in importance_tfidf[1]['bottom']]\n",
    "bottom_words = [a[1] for a in importance_tfidf[1]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try bag of word then tfidf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "train_text,val_text,train_labels,val_labels = train_test_split(labeled_cleaned,labels,test_size = 0.25,random_state = 0)\n",
    "X_train_count, count_vectorizer = cv(train_text)\n",
    "X_val_count = count_vectorizer.transform(val_text)\n",
    "\n",
    "# do tfidf to get X_test (unlabeled text that needed to be predicted)\n",
    "X_test_count = count_vectorizer.transform(unlabeled_cleaned) #transform on test set, not fit_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then do tfidf transformer to make training set and valid. set from occurences to freq.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tf = tfidf_transformer.fit_transform(X_train_count)\n",
    "X_val_tf = tfidf_transformer.transform(X_val_count)\n",
    "X_test_tf = tfidf_transformer.transform(X_test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 28  6  8 14  1  7 28 23  6 23 39 23 39  6 14 18 39 37 11 23 23 16  7\n",
      " 39 25 23  8 39 36 36 39 34 23 34 14  2 11 28 15 16  6 13 36 34 34 14 38\n",
      " 39 30 13 39 39 27  1 17  1 18  4 11 34 15 34 38 38 35 14 36  6  7 18 36\n",
      "  6 11 37  8 23 13 20 36  6  7  1 39  5 39 34 37 36 23 39 23 23 14 27 16\n",
      " 23 21 13  6]\n",
      "Test accuracy = 0.631, Train accuracy = 0.917,precision = 0.641, recall = 0.631, f1 = 0.623\n"
     ]
    }
   ],
   "source": [
    "# see the result in model LR\n",
    "predicted_labels,result,yHat_train=modelLR(X_train_tf,y_train,X_val_tf,X_test_tf)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Test accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the result in model XGBoost\n",
    "predicted_labels,result,yHat_train=modelxgb(X_train_tf,y_train,X_val_tf,X_test_tf)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Test accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we setup a pipeline and do some grid search on LR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "text_clf_LR = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf',TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression( random_state=0) ),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__penalty': ('l1','l2'),\n",
    "    'clf__C': (0.01,0.1,1,3,10,100),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.01, 0.1, 1, 3, 10, 100),\n",
      " 'clf__penalty': ('l1', 'l2'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed: 24.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1501.434s\n",
      "\n",
      "Best score: 0.620\n",
      "Best parameters set:\n",
      "\tclf__C: 100\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(text_clf_LR, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in text_clf_LR.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_text, train_labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_LR = Pipeline([('vect', CountVectorizer(max_df=1.0,ngram_range=(1,2))),\n",
    "                     ('tfidf',TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(C=100,penalty='l2',random_state=0)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6222222222222222"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_LR.fit(train_text, y_train)  \n",
    "predicted_ed2 = text_clf_LR.predict(val_text)\n",
    "np.mean(predicted_ed2 == y_val)    \n",
    "\n",
    "# which doesn't improve alot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try grid search on xgboost\n",
    "text_clf_xgb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf',TfidfTransformer()),\n",
    "                     ('clf', xgb.XGBClassifier(random_state=0) ),\n",
    " ])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_depth': (3,9,15),\n",
    "    'clf__alpha': (0,0.1,0.5,1),\n",
    "    'clf__Eta':(0.01,0.015,0.05,0.1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__Eta': (0.01, 0.015, 0.05, 0.1),\n",
      " 'clf__alpha': (0, 0.1, 0.5, 1),\n",
      " 'clf__max_depth': (3, 9, 15),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(text_clf_xgb, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in text_clf_xgb.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_text, train_labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# write result in csv\n",
    "with open('predictions.csv','w') as f:\n",
    "    f.write('Judgements' + '\\t' + 'Area of Law' + '\\n')\n",
    "    predictionList = all_labels[result]\n",
    "    for i in range(0, len(result)):\n",
    "        f.write(unlabeled[i] + '\\t' + predictionList[i] + '\\n')\n",
    "        \n",
    "sss = pd.read_csv('predictions.csv')\n",
    "print(sss)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
